{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 개요\n",
    "1. 크롤링 사이트 : 네이버쇼핑 - 네이버 페이탭\n",
    "2. 크롤링 데이터 : 상품명, url\n",
    "3. 1페이지부터 20페이지까지 순회하며 url 및 상품명 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib.parse import quote\n",
    "import chromedriver_autoinstaller\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depth2(계절가전), depth3(에어컨) 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name = ['계절가전','에어컨']\n",
    "    # 파일 저장에 사용\n",
    "urls = ['https://search.shopping.naver.com/search/category/100000581','https://search.shopping.naver.com/search/category/100000620#']\n",
    "    # depth2와 depth3의 페이지 링크 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20페이지까지 상품명, url 수집\n",
    "for i in range(2) :\n",
    "            # range(2) 대신 range(len(urls)) 사용해도 무방 <= url 리스트 안에 있는 url을 순회하며 페이지 크롤링 해오는 게 목적인 반복문\n",
    "    print(f\"{'-'*20} {cat_name[i]} {'-'*20}\")\n",
    "            # 현재 어떤 depth에서 수집하고 있는지 정보 표시\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = chrome_options) # 크롬드라이버 자동 인스톨, 안되면 직접 설치 필요\n",
    "    #driver = webdriver.Chrome(service = Service('C:/Users/user/Desktop/작업용 폴더/chromedriver-win64/chromedriver-win64/chromedriver.exe'), options = chrome_options)\n",
    "    driver.get(urls[i]) \n",
    "            # 계절가전, 에어컨 url 접근\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # 네이버 페이 클릭\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"content\"]/div[1]/div[1]/ul/li[3]/a').click()\n",
    "                                        # [네이버페이] 탭의 XPATH 값\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    page_nums = 1\n",
    "    prod_df = pd.DataFrame(columns = ['상품명','url'])\n",
    "                # 상품명, url 두 열을 가지고 있는 데이터 프레임 완성\n",
    "    while 1 :\n",
    "            # while 1(True) : 무한 반복문 처리\n",
    "        if page_nums == 21 :\n",
    "            # 20페이지까지만 크롤링\n",
    "            break\n",
    "            \n",
    "        SCROLL_PAUSE_SEC = 1\n",
    "\n",
    "        # 스크롤 높이 가져옴\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while 1:\n",
    "            # 끝까지 스크롤 다운 - 네이버 쇼핑 페이지가 스크롤을 내려야 추가정보를 가져오는 형태로 구성되어 있음\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # 1초 대기\n",
    "            time.sleep(SCROLL_PAUSE_SEC)\n",
    "\n",
    "            # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "                    # 더이상 스크롤이 내려가지 않을때까지 해당 루틴 반복\n",
    "            last_height = new_height\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        prod_list = soup.find_all('div',class_= 'product_title__Mmw2K')\n",
    "                                                    # 상품명 요소만 추출\n",
    "        for prod in prod_list :\n",
    "            url = str(prod).split('href=\"')[1].split()[0]\n",
    "            prod_nm = str(prod).split('title=')[1].split('>')[0].replace('\"','')\n",
    "            new_df = pd.DataFrame([[prod_nm,url]],columns = ['상품명','url'])\n",
    "            prod_df = pd.concat([prod_df,new_df])\n",
    "\n",
    "        \n",
    "        page_nums += 1\n",
    "        if page_nums == 21 :\n",
    "            break\n",
    "        elif page_nums-1 == 1 :\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"content\"]/div[1]/div[4]/a').click()\n",
    "        else :\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"content\"]/div[1]/div[4]/a[2]').click()\n",
    "        time.sleep(1)\n",
    "        print('page_nums:', page_nums-1, '수집 개수:',len(prod_df))\n",
    "\n",
    "    prod_df = prod_df.reset_index(drop = True)\n",
    "    file_name = f'{cat_name[i]}(네이버페이,800개).csv'\n",
    "    prod_df.to_csv(file_name, index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
