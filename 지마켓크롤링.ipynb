{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib.parse import quote\n",
    "import chromedriver_autoinstaller\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filePath 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './지마켓'\n",
    "os.makedirs(path)\n",
    "\n",
    "for i in range(2,4) :\n",
    "    folder_name = f'./지마켓/depth{i}'\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수집함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리마다 수집\n",
    "def crawl_gm(depth = 2, limit = 11) :\n",
    "    if depth == 2 :\n",
    "        # 상위 카테고리\n",
    "        gm_df = pd.DataFrame()\n",
    "        for i in range(1,201) :\n",
    "            driver.get('http://browse.gmarket.co.kr/list?category=200002920&gate_id=TVNUDGAX-O6HJ-ODF6-934Y-WI68HC1NF52L&s=8&k=0&p={}'.format(i))\n",
    "                        # 페이지별로 페이지url 정의\n",
    "            time.sleep(3)\n",
    "\n",
    "            html = driver.page_source\n",
    "                # 링크의 html 정보 스크랩핑\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "                # html에 있는 문자열만 모두 찾아서 출력\n",
    "            \n",
    "            # url 파서\n",
    "            urls = soup.select('a.link__item')\n",
    "            lst_list = []\n",
    "            cnt = 0 \n",
    "            for url in urls :\n",
    "                if cnt%2 == 0 :\n",
    "                        # href 중 허수 제거 (홀수번째 값만 파서)\n",
    "                    cnt += 1\n",
    "                    lst_list.append(url.attrs['href'])\n",
    "                else: \n",
    "                    cnt+= 1\n",
    "\n",
    "            # 상품명 파서\n",
    "            prdt_nm = soup.select('span.text__item')\n",
    "            tst_list = []\n",
    "            for prdt in prdt_nm :\n",
    "                tst_list.append(prdt.text.strip())\n",
    "                    # text 값만 가져옴\n",
    "\n",
    "            new_df = pd.DataFrame(lst_list,tst_list).reset_index()\n",
    "            new_df['페이지'] = len(tst_list)*[i]\n",
    "            new_df.columns = ['상품명','url','페이지']\n",
    "            gm_df = pd.concat([gm_df,new_df])\n",
    "\n",
    "            gm_df.drop_duplicates(['url'],keep = 'last', inplace = True)\n",
    "                # url 중복값 중 가장 마지막값만 남겨두고 나머지는 삭제\n",
    "            gm_df = gm_df.reset_index(drop = True)\n",
    "            print(f'{i}번째 페이지 수집 완료, 수집 개수 : {len(lst_list),len(prdt_nm)}')\n",
    "        gm_df.to_csv(f'G:/다른 컴퓨터/내 MacBook Air/MC/depth연구/3_지마켓(판매인기순)/세탁기/depth2/세탁기(지마켓,{len(gm_df)}개).csv', index = False, encoding = 'utf-8-sig')\n",
    "        \n",
    "    else :\n",
    "        # 하위 카테고리\n",
    "        gm_df = pd.DataFrame()\n",
    "        urls = f'{driver.current_url}'\n",
    "        for i in range(1,limit) :            \n",
    "            driver.get(f'{driver.current_url}&k=0&p={i}')\n",
    "                # 현재 url에 추가적인 쿼리 매개변수를 포합한 새로운 url로 이동\n",
    "            time.sleep(3)\n",
    "\n",
    "            html = driver.page_source\n",
    "                    # 현재 웹페이지의 HTML 소스코드를 가져옴 -> 파싱\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            urls = soup.select('a.link__item')\n",
    "                                    # 'a' 태그 중 클래스가 'link__item'인 요소 선택\n",
    "            lst_list = []\n",
    "\n",
    "            cnt = 0 \n",
    "            for url in urls :\n",
    "                if cnt%2 == 0 :\n",
    "                        # 짝수번째 url만 수집\n",
    "                    cnt += 1\n",
    "                    lst_list.append(url.attrs['href'])\n",
    "                else: \n",
    "                    cnt+= 1\n",
    "                    \n",
    "            # 상품명\n",
    "            prdt_nm = soup.select('span.text__item')\n",
    "                                    # 상품명 : 'span' 태그 중 클래스가 'text__item'인 요소들\n",
    "            tst_list = []\n",
    "            for prdt in prdt_nm :\n",
    "                tst_list.append(prdt.text.strip())\n",
    "\n",
    "            new_df = pd.DataFrame(lst_list,tst_list).reset_index()\n",
    "            new_df['페이지'] = len(tst_list)*[i]\n",
    "            new_df.columns = ['상품명','url','페이지']\n",
    "            gm_df = pd.concat([gm_df,new_df])\n",
    "\n",
    "            gm_df.drop_duplicates(['url'],keep = 'last', inplace = True)\n",
    "            gm_df = gm_df.reset_index(drop = True)\n",
    "            print(f'{i}번째 페이지 수집 완료, 수집 개수 : {len(lst_list),len(prdt_nm)}')\n",
    "        return gm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
